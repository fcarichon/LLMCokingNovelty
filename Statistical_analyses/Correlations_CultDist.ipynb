{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362d0c7-e344-4926-a9fc-5b17291cb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from os import walk\n",
    "\n",
    "from Stats_utils import *\n",
    "from langdetect import detect\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96539b55",
   "metadata": {},
   "source": [
    "## RQ 2 -- LLMs relations to cultural distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1fb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b284d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(recette, thrs=20):\n",
    "    list_recette = recette.split()\n",
    "    if len(list_recette) > thrs:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4670b31-2e5c-494b-a014-573a9da792bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = '.'\n",
    "my_path = f\"./LLM_scored/config_run/\"\n",
    "filenames = list(set(next(walk(my_path), (None,None,[]))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cde92-4c19-4cae-a1a4-a16394bda799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/f/florian.carichon/.conda/envs/work0/lib/python3.10/site-packages/openpyxl/worksheet/_read_only.py:85: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "## LOADING CULTURAL DISTANCES DATASETS\n",
    "df_ingle = pd.read_excel(f'./CulturalDistanceEVSWVS.xlsx')\n",
    "df_ingle = df_ingle.dropna(subset=['Country'])\n",
    "dist_ingle = inglehart_dist(df_ingle)\n",
    "\n",
    "df_geo = pd.read_excel(f'./Country_latLong.xlsx')\n",
    "df_geo = df_geo.dropna(subset=['Country'])\n",
    "dist_geo = geograph_dist(df_geo)\n",
    "\n",
    "dist_ling = pd.read_excel(f'./Lang-Dist_all-years_n22350.xlsx')\n",
    "dist_rel = pd.read_csv(f'./religious_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changer for the propoer analyzed layers\n",
    "layer_list = ['layer_original', 'layer_1', 'layer_23', 'layer_-1', 'layer_last']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad662cf",
   "metadata": {},
   "source": [
    "## Measuring Cultural correaltions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0b09a-ac50-4dff-acc4-fbc1350fa8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [16:48<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initializing all metrics we want to save for correlation and logistic regression analyses\n",
    "all_new_div, all_new_prob, all_new_extr, all_new_rank = [], [], [], []\n",
    "all_uniq_dist, all_uniq_proto, all_diff_glob, all_diff_loc = [], [], [], []\n",
    "all_nsurp, all_dsurp = [], []\n",
    "all_dist_ingle, all_dist_ling, all_dist_geo, all_dist_rel = [], [], [], []\n",
    "nb_ingr, nb_new_ingr, ratio_new_ingr, clean_text_length, clean_nb_uniq_tokens, lexical_div, ratio_length = [], [], [], [], [], [], []\n",
    "\n",
    "#FOR LINGUISTIC OR RELIGIOUS DISTANCE\n",
    "country_list_1 = list(set(dist_ling['country_i']))\n",
    "country_list_2 = list(set(dist_rel['country_i']))\n",
    "#FOR INGLEHART CULTURAl or GEOGRAPHICAL DISTANCE\n",
    "country_list_3 = list(df_geo['Country'])\n",
    "country_list_4 = list(df_ingle['Country'])\n",
    "\n",
    "for k in tqdm(range(len(filenames))):\n",
    "    filename = filenames[k]\n",
    "    file_path = my_path + filename\n",
    "    with open(file_path) as json_file:\n",
    "        recipe_dict1 = json.load(json_file)\n",
    "\n",
    "    \n",
    "    country = recipe_dict1['Country']\n",
    "    ##Exceptions due to the Congo Bars recipes\n",
    "    if country == 'congo':\n",
    "        country = 'united states'\n",
    "\n",
    "    ### Uncomment here for linguistic and relgion analyses\n",
    "    # if country in country_list_1:\n",
    "    #     country_dist1 = dist_ling[dist_ling['country_i'] == country]\n",
    "    #     country_dist2 = dist_rel[dist_rel['country_i'] == country]\n",
    "        #print(country_dist2)\n",
    "    ## Uncomment here for cultural and geographical distances\n",
    "    if country in country_list_3 and country in country_list_4:\n",
    "        country_dist3 = dist_geo[country]\n",
    "        country_dist4 = dist_ingle[country]\n",
    "\n",
    "        variation_list = list(recipe_dict1['LLM_gen']['variation_novelty'].keys())\n",
    "        varia_country_list = [key.split(\"_\")[1] for key in variation_list]\n",
    "\n",
    "        for i, variation in enumerate(variation_list):\n",
    "            varia_country = varia_country_list[i]\n",
    "            if varia_country == 'taiwan': ## For geo only\n",
    "                varia_country = 'japan' \n",
    "            ### Uncomment here for linguistic and relgion analyses\n",
    "            # if varia_country in country_list_1:\n",
    "            if varia_country in country_list_3 and varia_country in country_list_4:\n",
    "                ingle_dist = get_distance_LLMs(varia_country, country_dist3, type='ingle')\n",
    "                geo_dist = get_distance_LLMs(varia_country, country_dist4, type='geo')\n",
    "                ### Uncomment here for linguistic and relgion analyses\n",
    "                # ling_dist = get_distance_LLMs(varia_country, country_dist1)\n",
    "                # reli_dist = get_distance_LLMs(varia_country, country_dist2, type='reli')\n",
    "                list_recipe_var = list(recipe_dict1['LLM_gen']['variation_novelty'][variation].keys())\n",
    "                for recipe in list_recipe_var:\n",
    "                    is_eng = is_english(recipe_dict1['LLM_gen']['variation_novelty'][variation][recipe]['Instructions'])\n",
    "                    if is_eng:\n",
    "                        count_ = count_tokens(recipe_dict1['LLM_gen']['variation_novelty'][variation][recipe]['Instructions'])\n",
    "                        if count_:\n",
    "                            all_dist_ingle.append(ingle_dist)\n",
    "                            all_dist_geo.append(geo_dist)\n",
    "                            ### Uncomment here for linguistic and relgion analyses\n",
    "                            # all_dist_ling.append(ling_dist)\n",
    "                            # all_dist_rel.append(reli_dist)\n",
    "                            all_new_div.append(recipe_dict1['LLM_gen']['variation_novelty'][variation][recipe]['layer_original']['newness_div'])\n",
    "                            all_uniq_dist.append(recipe_dict1['LLM_gen']['variation_novelty'][variation][recipe]['layer_original']['uniq_dist'])\n",
    "                            all_diff_loc.append(recipe_dict1['LLM_gen']['variation_novelty'][variation][recipe]['layer_original']['diff_local'])\n",
    "                            all_nsurp.append(recipe_dict1['LLM_gen']['variation_novelty'][variation][recipe]['layer_original']['new_surprise'])\n",
    "                            all_dsurp.append(recipe_dict1['LLM_gen']['variation_novelty'][variation][recipe]['layer_original']['dist_surprise'])\n",
    "\n",
    "data = {\n",
    "    'newness_div': all_new_div,\n",
    "    'uniq_dist': all_uniq_dist,\n",
    "    'diff_local': all_diff_loc,\n",
    "    'new_surprise': all_nsurp,\n",
    "    'dist_surprise': all_dsurp,\n",
    "    'dist_ingle': all_dist_ingle,\n",
    "    #'dist_ling': all_dist_ling,\n",
    "    'dist_geo': all_dist_geo,\n",
    "}\n",
    "\n",
    "# Filter out keys where the list is empty\n",
    "filtered_data = {k: v for k, v in data.items() if len(v) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c3870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148474 0 0\n"
     ]
    }
   ],
   "source": [
    "#Control purpose\n",
    "print(len(all_new_div), len(all_dist_ling), len(all_dist_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW :  0.004 ( 0.09 )\n",
      "UNIQ :  -0.031 ( 0.0 )\n",
      "DIFF :  -0.009 ( 0.0 )\n",
      "NSURP :  -0.029 ( 0.0 )\n",
      "DSURP :  0.029 ( 0.0 )\n",
      "========================================================================\n",
      "NEW :  -0.029 ( 0.0 )\n",
      "UNIQ :  0.021 ( 0.0 )\n",
      "DIFF :  -0.004 ( 0.11 )\n",
      "BSURP :  0.008 ( 0.001 )\n",
      "DSURP :  0.004 ( 0.13 )\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "#FOR CULTURAL AND GEOGRAPHICAL ONLY (SEE BELOW FOR LINGUISTIC)\n",
    "correlation_new, p_value_new = pearsonr(all_dist_ingle, all_new_div)\n",
    "correlation_uniq, p_value_uniq = pearsonr(all_dist_ingle, all_uniq_dist)\n",
    "correlation_diff, p_value_diff = pearsonr(all_dist_ingle, all_diff_loc)\n",
    "correlation_nsurp, p_value_nsurp = pearsonr(all_dist_ingle, all_nsurp)\n",
    "correlation_dsurp, p_value_dsurp = pearsonr(all_dist_ingle, all_dsurp)\n",
    "\n",
    "### Uncomment here for Spearman correlations\n",
    "# correlation_new, p_value_new = spearmanr(all_dist_ingle, all_new_div)\n",
    "# correlation_uniq, p_value_uniq = spearmanr(all_dist_ingle, all_uniq_dist)\n",
    "# correlation_diff, p_value_diff = spearmanr(all_dist_ingle, all_diff_loc)\n",
    "# correlation_nsurp, p_value_nsurp = spearmanr(all_dist_ingle, all_nsurp)\n",
    "# correlation_dsurp, p_value_dsurp = spearmanr(all_dist_ingle, all_dsurp)\n",
    "\n",
    "print('NEW : ', round(correlation_new,3), '(',round(p_value_new, 3),')')\n",
    "print('UNIQ : ', round(correlation_uniq,3), '(',round(p_value_uniq, 3),')')\n",
    "print('DIFF : ', round(correlation_diff,3), '(',round(p_value_diff, 3),')')\n",
    "print('NSURP : ', round(correlation_nsurp,3), '(',round(p_value_nsurp, 3),')')\n",
    "print('DSURP : ', round(correlation_dsurp,3), '(',round(p_value_dsurp, 3),')')\n",
    "print('========================================================================')\n",
    "\n",
    "\n",
    "correlation_new, p_value_new = pearsonr(all_dist_geo, all_new_div)\n",
    "correlation_uniq, p_value_uniq = pearsonr(all_dist_geo, all_uniq_dist)\n",
    "correlation_diff, p_value_diff = pearsonr(all_dist_geo, all_diff_loc)\n",
    "correlation_nsurp, p_value_nsurp = pearsonr(all_dist_geo, all_nsurp)\n",
    "correlation_dsurp, p_value_dsurp = pearsonr(all_dist_geo, all_dsurp)\n",
    "\n",
    "### Uncomment here for Spearman correlations\n",
    "# correlation_new, p_value_new = spearmanr(all_dist_geo, all_new_div)\n",
    "# correlation_uniq, p_value_uniq = spearmanr(all_dist_geo, all_uniq_dist)\n",
    "# correlation_diff, p_value_diff = spearmanr(all_dist_geo, all_diff_loc)\n",
    "# correlation_nsurp, p_value_nsurp = spearmanr(all_dist_geo, all_nsurp)\n",
    "# correlation_dsurp, p_value_dsurp = spearmanr(all_dist_geo, all_dsurp)\n",
    "\n",
    "print('NEW : ', round(correlation_new,3), '(',round(p_value_new, 3),')')\n",
    "print('UNIQ : ', round(correlation_uniq,3), '(',round(p_value_uniq, 3),')')\n",
    "print('DIFF : ', round(correlation_diff,3), '(',round(p_value_diff, 3),')')\n",
    "print('BSURP : ', round(correlation_nsurp,3), '(',round(p_value_nsurp, 3),')')\n",
    "print('DSURP : ', round(correlation_dsurp,3), '(',round(p_value_dsurp, 3),')')\n",
    "print('========================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd184c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW :  -0.06 ( 0.0 )\n",
      "UNIQ :  0.018 ( 0.0 )\n",
      "DIFF :  -0.014 ( 0.0 )\n",
      "NSURP :  -0.008 ( 0.001 )\n",
      "DSURP :  -0.001 ( 0.765 )\n",
      "========================================================================\n",
      "NEW :  -0.01 ( 0.0 )\n",
      "UNIQ :  -0.017 ( 0.0 )\n",
      "DIFF :  -0.009 ( 0.0 )\n",
      "BSURP :  -0.009 ( 0.0 )\n",
      "DSURP :  0.046 ( 0.0 )\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "#FOR LINGUISITIC AND RELIGIOUS\n",
    "\n",
    "correlation_new, p_value_new = pearsonr(all_dist_ling, all_new_div)\n",
    "correlation_uniq, p_value_uniq = pearsonr(all_dist_ling, all_uniq_dist)\n",
    "correlation_diff, p_value_diff = pearsonr(all_dist_ling, all_diff_loc)\n",
    "correlation_nsurp, p_value_nsurp = pearsonr(all_dist_ling, all_nsurp)\n",
    "correlation_dsurp, p_value_dsurp = pearsonr(all_dist_ling, all_dsurp)\n",
    "\n",
    "# correlation_new, p_value_new = spearmanr(all_dist_ling, all_new_div)\n",
    "# correlation_uniq, p_value_uniq = spearmanr(all_dist_ling, all_uniq_dist)\n",
    "# correlation_diff, p_value_diff = spearmanr(all_dist_ling, all_diff_loc)\n",
    "# correlation_nsurp, p_value_nsurp = spearmanr(all_dist_ling, all_nsurp)\n",
    "# correlation_dsurp, p_value_dsurp = spearmanr(all_dist_ling, all_dsurp)\n",
    "\n",
    "print('NEW : ', round(correlation_new,3), '(',round(p_value_new, 3),')')\n",
    "print('UNIQ : ', round(correlation_uniq,3), '(',round(p_value_uniq, 3),')')\n",
    "print('DIFF : ', round(correlation_diff,3), '(',round(p_value_diff, 3),')')\n",
    "print('NSURP : ', round(correlation_nsurp,3), '(',round(p_value_nsurp, 3),')')\n",
    "print('DSURP : ', round(correlation_dsurp,3), '(',round(p_value_dsurp, 3),')')\n",
    "print('========================================================================')\n",
    "\n",
    "\n",
    "correlation_new, p_value_new = pearsonr(all_dist_rel, all_new_div)\n",
    "correlation_uniq, p_value_uniq = pearsonr(all_dist_rel, all_uniq_dist)\n",
    "correlation_diff, p_value_diff = pearsonr(all_dist_rel, all_diff_loc)\n",
    "correlation_nsurp, p_value_nsurp = pearsonr(all_dist_rel, all_nsurp)\n",
    "correlation_dsurp, p_value_dsurp = pearsonr(all_dist_rel, all_dsurp)\n",
    "\n",
    "# correlation_new, p_value_new = spearmanr(all_dist_rel, all_new_div)\n",
    "# correlation_uniq, p_value_uniq = spearmanr(all_dist_rel, all_uniq_dist)\n",
    "# correlation_diff, p_value_diff = spearmanr(all_dist_rel, all_diff_loc)\n",
    "# correlation_nsurp, p_value_nsurp = spearmanr(all_dist_rel, all_nsurp)\n",
    "# correlation_dsurp, p_value_dsurp = spearmanr(all_dist_rel, all_dsurp)\n",
    "\n",
    "print('NEW : ', round(correlation_new,3), '(',round(p_value_new, 3),')')\n",
    "print('UNIQ : ', round(correlation_uniq,3), '(',round(p_value_uniq, 3),')')\n",
    "print('DIFF : ', round(correlation_diff,3), '(',round(p_value_diff, 3),')')\n",
    "print('BSURP : ', round(correlation_nsurp,3), '(',round(p_value_nsurp, 3),')')\n",
    "print('DSURP : ', round(correlation_dsurp,3), '(',round(p_value_dsurp, 3),')')\n",
    "print('========================================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
