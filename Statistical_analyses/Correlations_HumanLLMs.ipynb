{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from os import walk\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30415b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n"
     ]
    }
   ],
   "source": [
    "data_path = \".LLM_Scored/run_config/\"\n",
    "filenames = list(set(next(walk(data_path), (None,None,[]))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1ef444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77a8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(recette, thrs=20):\n",
    "    list_recette = recette.split()\n",
    "    if len(list_recette) > thrs:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84520354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change layer names based on the analyzed layers you kept in measuring novelty\n",
    "layer_list = ['layer_original', 'layer_1', 'layer_23', 'layer_-2', 'layer_-1', 'layer_last']\n",
    "tradi_list = ['authenticity', 'tradition', 'prototype']\n",
    "novel_list = ['novelty', 'uniquness', 'newness', 'difference', 'surprise', 'creativity', 'originality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2def280",
   "metadata": {},
   "source": [
    "# Measuring divergence between same coutries from last layers first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c220113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [02:36<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Analyzed Layer :  layer_original\n",
      "Mean Human Newness: 0.0027830668916057387 Mean LLM Newness: 0.006895570557044753\n",
      "Mean Human Uniq: 0.38316307164409147 Mean LLM Uniq: 0.5399983494708731\n",
      "Mean Human Difference: 0.5296523517382413 Mean LLM Difference: 0.9959378089332256\n",
      "Mean Human Newn Surprsie: 0.5006257320591598 Mean Newn Surprsie: 0.972153433699343\n",
      "Mean Human Dist Surprise: 0.3975102562490127 Mean LLM Dist Surprise: 0.5528488388328215\n",
      "t-test p-value NEwness: 0.0\n",
      "t-test p-value Uniq: 0.0\n",
      "t-test p-value Difference: 0.0\n",
      "t-test p-value New Surprise: 0.0\n",
      "t-test p-value Dist Surpr: 0.0\n",
      "Mann–Whitney p-value NEwness: 2.396107279482812e-54\n",
      "Mann–Whitney p-value Uniq: 0.0\n",
      "Mann–Whitney p-value Difference: 0.0\n",
      "Mann–Whitney p-value New Surpr: 0.0\n",
      "Mann–Whitney p-value Dist Surpr: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [02:34<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Analyzed Layer :  layer_1\n",
      "Mean Human Newness: 0.0011134519360255714 Mean LLM Newness: 0.0047753306025139245\n",
      "Mean Human Uniq: 0.5318626445270651 Mean LLM Uniq: 0.6508539090760618\n",
      "Mean Human Difference: 0.8800990205575288 Mean LLM Difference: 0.9707679588792438\n",
      "Mean Human Newn Surprsie: 0.6689474789555829 Mean Newn Surprsie: 0.9865923903205233\n",
      "Mean Human Dist Surprise: 0.2657080184220042 Mean LLM Dist Surprise: 0.19529846689392794\n",
      "t-test p-value NEwness: 0.0\n",
      "t-test p-value Uniq: nan\n",
      "t-test p-value Difference: 3.6672130849021064e-63\n",
      "t-test p-value New Surprise: 0.0\n",
      "t-test p-value Dist Surpr: 1.564782092847612e-106\n",
      "Mann–Whitney p-value NEwness: 0.0\n",
      "Mann–Whitney p-value Uniq: nan\n",
      "Mann–Whitney p-value Difference: 7.361994995773585e-241\n",
      "Mann–Whitney p-value New Surpr: 0.0\n",
      "Mann–Whitney p-value Dist Surpr: 2.7541372599346232e-102\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [02:35<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Analyzed Layer :  layer_23\n",
      "Mean Human Newness: 0.0010844807988022974 Mean LLM Newness: 0.006536891136844302\n",
      "Mean Human Uniq: 0.5810869179608331 Mean LLM Uniq: 0.6282530067890991\n",
      "Mean Human Difference: 0.9141104294478528 Mean LLM Difference: 0.9701642762140982\n",
      "Mean Human Newn Surprsie: 0.7709440753966821 Mean Newn Surprsie: 0.9917455296507125\n",
      "Mean Human Dist Surprise: 0.259660419411083 Mean LLM Dist Surprise: 0.361518421996419\n",
      "t-test p-value NEwness: 0.0\n",
      "t-test p-value Uniq: nan\n",
      "t-test p-value Difference: 2.0698918044079486e-33\n",
      "t-test p-value New Surprise: 0.0\n",
      "t-test p-value Dist Surpr: 1.1192252379243955e-167\n",
      "Mann–Whitney p-value NEwness: 0.0\n",
      "Mann–Whitney p-value Uniq: nan\n",
      "Mann–Whitney p-value Difference: 2.6872111862390363e-117\n",
      "Mann–Whitney p-value New Surpr: 0.0\n",
      "Mann–Whitney p-value Dist Surpr: 7.703856941856106e-186\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [02:38<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Analyzed Layer :  layer_-2\n",
      "Mean Human Newness: 0.0009707152220723341 Mean LLM Newness: 0.004498949556876946\n",
      "Mean Human Uniq: 0.6558507597793434 Mean LLM Uniq: 0.6558048387493313\n",
      "Mean Human Difference: 0.9187385642019159 Mean LLM Difference: 0.9642131751595219\n",
      "Mean Human Newn Surprsie: 0.7949461669975111 Mean Newn Surprsie: 0.9833572487678658\n",
      "Mean Human Dist Surprise: 0.07433140632772477 Mean LLM Dist Surprise: 0.1873133262842399\n",
      "t-test p-value NEwness: 0.0\n",
      "t-test p-value Uniq: nan\n",
      "t-test p-value Difference: 8.082221655431828e-23\n",
      "t-test p-value New Surprise: 5.645498961894304e-240\n",
      "t-test p-value Dist Surpr: 0.0\n",
      "Mann–Whitney p-value NEwness: 0.0\n",
      "Mann–Whitney p-value Uniq: nan\n",
      "Mann–Whitney p-value Difference: 9.158351119375173e-48\n",
      "Mann–Whitney p-value New Surpr: 0.0\n",
      "Mann–Whitney p-value Dist Surpr: 4.042618085138591e-254\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [02:35<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Analyzed Layer :  layer_-1\n",
      "Mean Human Newness: 0.0012129618228646616 Mean LLM Newness: 0.00428551255940128\n",
      "Mean Human Uniq: 0.49806214123126247 Mean LLM Uniq: 0.5833921262727564\n",
      "Mean Human Difference: 0.8984501130125928 Mean LLM Difference: 0.969010009524323\n",
      "Mean Human Newn Surprsie: 0.6953137337947763 Mean Newn Surprsie: 0.9723626361486797\n",
      "Mean Human Dist Surprise: 0.39455311148186867 Mean LLM Dist Surprise: 0.4825518588014746\n",
      "t-test p-value NEwness: 0.0\n",
      "t-test p-value Uniq: nan\n",
      "t-test p-value Difference: 1.9522661738470163e-46\n",
      "t-test p-value New Surprise: 0.0\n",
      "t-test p-value Dist Surpr: 4.439125065451348e-147\n",
      "Mann–Whitney p-value NEwness: 0.0\n",
      "Mann–Whitney p-value Uniq: nan\n",
      "Mann–Whitney p-value Difference: 1.789761920416219e-170\n",
      "Mann–Whitney p-value New Surpr: 0.0\n",
      "Mann–Whitney p-value Dist Surpr: 1.3785588319798174e-296\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [02:36<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Analyzed Layer :  layer_last\n",
      "Mean Human Newness: 0.0018506114360352247 Mean LLM Newness: 0.005482207195024729\n",
      "Mean Human Uniq: 0.40634365295680014 Mean LLM Uniq: 0.508727226397594\n",
      "Mean Human Difference: 0.5666774297707459 Mean LLM Difference: 0.9322405550163558\n",
      "Mean Human Newn Surprsie: 0.5171211364008291 Mean Newn Surprsie: 0.9005393569462083\n",
      "Mean Human Dist Surprise: 0.39868023786889584 Mean LLM Dist Surprise: 0.5127719431603094\n",
      "t-test p-value NEwness: 0.0\n",
      "t-test p-value Uniq: nan\n",
      "t-test p-value Difference: 0.0\n",
      "t-test p-value New Surprise: 0.0\n",
      "t-test p-value Dist Surpr: 7.395684855468558e-306\n",
      "Mann–Whitney p-value NEwness: 0.0\n",
      "Mann–Whitney p-value Uniq: nan\n",
      "Mann–Whitney p-value Difference: 0.0\n",
      "Mann–Whitney p-value New Surpr: 0.0\n",
      "Mann–Whitney p-value Dist Surpr: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in layer_list:\n",
    "    #For the country of origin\n",
    "    human_new_orig, LLM_new_orig, human_new_varia, LLM_new_varia = [], [], [], []\n",
    "    human_uniq_orig, LLM_uniq_orig, human_uniq_varia, LLM_uniq_varia = [], [], [], []\n",
    "    human_diff_orig, LLM_diff_orig, human_diff_varia, LLM_diff_varia = [], [], [], []\n",
    "    human_nsurp_orig, LLM_nsurp_orig, human_nsurp_varia, LLM_nsurp_varia = [], [], [], []\n",
    "    human_dsurp_orig, LLM_dsurp_orig, human_dsurp_varia, LLM_dsurp_varia = [], [], [], []\n",
    "    for k in tqdm(range(len(filenames))):\n",
    "        recette = filenames[k]\n",
    "        file_path = data_path + recette\n",
    "        with open(file_path) as json_file:\n",
    "            recipe_dict = json.load(json_file)\n",
    "\n",
    "        country_orig = recipe_dict['Country'].lower()\n",
    "        train_recipe_ids = list(recipe_dict['Train_Variations'].keys())\n",
    "        for i in range(len(train_recipe_ids)):\n",
    "            recipe = recipe_dict['Train_Variations'][train_recipe_ids[i]]\n",
    "            if recipe['country'].lower() == country_orig:\n",
    "                human_new_orig.append(recipe[layer]['newness_div'])\n",
    "                human_uniq_orig.append(recipe[layer]['uniq_dist'])\n",
    "                human_diff_orig.append(recipe[layer]['diff_local'])\n",
    "                human_nsurp_orig.append(recipe[layer]['new_surprise'])\n",
    "                human_nsurp_orig.append(recipe[layer]['dist_surprise'])\n",
    "                \n",
    "        valid_recipe_ids = list(recipe_dict['Valid_Variations'].keys())\n",
    "        for i in range(len(valid_recipe_ids)):\n",
    "            recipe = recipe_dict['Valid_Variations'][valid_recipe_ids[i]]\n",
    "            if recipe['country'].lower() == country_orig:\n",
    "                human_new_orig.append(recipe[layer]['newness_div'])\n",
    "                human_uniq_orig.append(recipe[layer]['uniq_dist'])\n",
    "                human_diff_orig.append(recipe[layer]['diff_local'])\n",
    "                human_nsurp_orig.append(recipe[layer]['new_surprise'])\n",
    "                human_dsurp_orig.append(recipe[layer]['dist_surprise'])\n",
    "\n",
    "        test_recipe_ids = list(recipe_dict['Test_Variations'].keys())\n",
    "        for i in range(len(test_recipe_ids)):\n",
    "            recipe = recipe_dict['Test_Variations'][test_recipe_ids[i]]\n",
    "            if recipe['country'].lower() == country_orig:\n",
    "                human_new_orig.append(recipe[layer]['newness_div'])\n",
    "                human_uniq_orig.append(recipe[layer]['uniq_dist'])\n",
    "                human_diff_orig.append(recipe[layer]['diff_local'])\n",
    "                human_nsurp_orig.append(recipe[layer]['new_surprise'])\n",
    "                human_dsurp_orig.append(recipe[layer]['dist_surprise'])\n",
    "                \n",
    "    ### KEEP THIS PART WHEN YOU WANT TO OBSERVE THE CULTURAL DISTANCE -- TO WHICH TYPE OF COUNTRY IT CORRESPOND?? \n",
    "        if 'same_country_novelty' in recipe_dict['LLM_gen']:\n",
    "            llm_recipe_ids = list(recipe_dict['LLM_gen']['same_country_novelty'].keys())\n",
    "            for i in range(len(llm_recipe_ids)):\n",
    "                tradi_text = llm_recipe_ids[i].split('_')[-1]\n",
    "                #if tradi_text in tradi_list: \n",
    "                recipe = recipe_dict['LLM_gen']['same_country_novelty'][llm_recipe_ids[i]]\n",
    "                is_eng = is_english(recipe['Instructions'])\n",
    "                if is_eng:\n",
    "                    count_ = count_tokens(recipe['Instructions'])\n",
    "                    if count_:\n",
    "                        LLM_new_orig.append(recipe[layer]['newness_div'])\n",
    "                        LLM_uniq_orig.append(recipe[layer]['uniq_dist'])\n",
    "                        LLM_diff_orig.append(recipe[layer]['diff_local'])\n",
    "                        LLM_nsurp_orig.append(recipe[layer]['new_surprise'])\n",
    "                        LLM_dsurp_orig.append(recipe[layer]['dist_surprise'])\n",
    "\n",
    "    print('===============================')\n",
    "    print('Analyzed Layer : ', layer)\n",
    "    x0 = np.array(human_new_orig)\n",
    "    y0 = np.array(LLM_new_orig)\n",
    "    x1 = np.array(human_uniq_orig)\n",
    "    y1 = np.array(LLM_uniq_orig)\n",
    "    x2 = np.array(human_diff_orig)\n",
    "    y2 = np.array(LLM_diff_orig)\n",
    "    x3 = np.array(human_nsurp_orig)\n",
    "    y3 = np.array(LLM_nsurp_orig)\n",
    "    x4 = np.array(human_dsurp_orig)\n",
    "    y4 = np.array(LLM_dsurp_orig)\n",
    "\n",
    "    # Step 1: Mean values\n",
    "    mean_x0, mean_y0 = np.mean(x0), np.mean(y0)\n",
    "    print(\"Mean Human Newness:\", mean_x0, \"Mean LLM Newness:\", mean_y0)\n",
    "    mean_x1, mean_y1 = np.nanmean(x1), np.nanmean(y1)\n",
    "    print(\"Mean Human Uniq:\", mean_x1, \"Mean LLM Uniq:\", mean_y1)\n",
    "    mean_x2, mean_y2 = np.nanmean(x2), np.nanmean(y2)\n",
    "    print(\"Mean Human Difference:\", mean_x2, \"Mean LLM Difference:\", mean_y2)\n",
    "    mean_x3, mean_y3 = np.mean(x3), np.mean(y3)\n",
    "    print(\"Mean Human Newn Surprsie:\", mean_x3, \"Mean Newn Surprsie:\", mean_y3)\n",
    "    mean_x4, mean_y4 = np.mean(x4), np.mean(y4)\n",
    "    print(\"Mean Human Dist Surprise:\", mean_x4, \"Mean LLM Dist Surprise:\", mean_y4)\n",
    "\n",
    "    # Step 2a: Independent t-test (Welch’s by default with equal_var=False)\n",
    "    t_stat0, p_val_t0 = stats.ttest_ind(x0, y0, equal_var=False)\n",
    "    print(\"t-test p-value NEwness:\", p_val_t0)\n",
    "    t_stat1, p_val_t1 = stats.ttest_ind(x1, y1, equal_var=False)\n",
    "    print(\"t-test p-value Uniq:\", p_val_t1)\n",
    "    t_stat2, p_val_t2 = stats.ttest_ind(x2, y2, equal_var=False)\n",
    "    print(\"t-test p-value Difference:\", p_val_t2)\n",
    "    t_stat3, p_val_t3 = stats.ttest_ind(x3, y3, equal_var=False)\n",
    "    print(\"t-test p-value New Surprise:\", p_val_t3)\n",
    "    t_stat4, p_val_t4 = stats.ttest_ind(x4, y4, equal_var=False)\n",
    "    print(\"t-test p-value Dist Surpr:\", p_val_t4)\n",
    "\n",
    "    # Step 2b: Mann–Whitney U test\n",
    "    u_stat0, p_val_u0 = stats.mannwhitneyu(x0, y0, alternative='two-sided')\n",
    "    print(\"Mann–Whitney p-value NEwness:\", p_val_u0)\n",
    "    # Step 2b: Mann–Whitney U test\n",
    "    u_stat1, p_val_u1 = stats.mannwhitneyu(x1, y1, alternative='two-sided')\n",
    "    print(\"Mann–Whitney p-value Uniq:\", p_val_u1)\n",
    "    # Step 2b: Mann–Whitney U test\n",
    "    u_stat2, p_val_u2 = stats.mannwhitneyu(x2, y2, alternative='two-sided')\n",
    "    print(\"Mann–Whitney p-value Difference:\", p_val_u2)\n",
    "    # Step 2b: Mann–Whitney U test\n",
    "    u_stat3, p_val_u3 = stats.mannwhitneyu(x3, y3, alternative='two-sided')\n",
    "    print(\"Mann–Whitney p-value New Surpr:\", p_val_u3)\n",
    "    # Step 2b: Mann–Whitney U test\n",
    "    u_stat4, p_val_u4 = stats.mannwhitneyu(x4, y4, alternative='two-sided')\n",
    "    print(\"Mann–Whitney p-value Dist Surpr:\", p_val_u4)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Human: 0.14932028862994123 Mean LLM: 0.19266642040984727\n",
      "t-test p-value: 7.872046782369342e-23\n",
      "Mann–Whitney p-value: 2.9896601324397093e-24\n"
     ]
    }
   ],
   "source": [
    "#T-TEST BETWEEN HUMAN AND LLMs\n",
    "x = np.array(human_dsurp_orig)\n",
    "y = np.array(LLM_dsurp_orig)\n",
    "\n",
    "# Step 1: Mean values\n",
    "mean_x, mean_y = np.mean(x), np.mean(y)\n",
    "print(\"Mean Human:\", mean_x, \"Mean LLM:\", mean_y)\n",
    "\n",
    "# Step 2a: Independent t-test (Welch’s by default with equal_var=False)\n",
    "t_stat, p_val_t = stats.ttest_ind(x, y, equal_var=False)\n",
    "print(\"t-test p-value:\", p_val_t)\n",
    "\n",
    "# Step 2b: Mann–Whitney U test\n",
    "u_stat, p_val_u = stats.mannwhitneyu(x, y, alternative='two-sided')\n",
    "print(\"Mann–Whitney p-value:\", p_val_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5564798e",
   "metadata": {},
   "source": [
    "# For the variation countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538db603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/495 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [18:56<00:00,  2.30s/it]\n",
      "/home/mila/f/florian.carichon/.conda/envs/work0/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:586: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/mila/f/florian.carichon/.conda/envs/work0/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/mila/f/florian.carichon/.conda/envs/work0/lib/python3.10/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Layer :  layer_original\n",
      "Newness Human Mean :  0.0028484571437995872\n",
      "Newness LLM Mean :  0.0079112853936784\n",
      "Newness ttest:  0.007620622987283007\n",
      "Newness utest : 0.2222783859267937\n",
      "----\n",
      "Uniq Human Mean :  0.41785444088924406\n",
      "Uniq LLM Mean :  0.5387996758920184\n",
      "Uniq ttest:  0.02603535129901908\n",
      "Uniq utest : 0.03681350864696227\n",
      "----\n",
      "Diff Human Mean :  0.7547152980650726\n",
      "Diff LLM Mean :  0.9944571175345944\n",
      "Diff ttest:  nan\n",
      "Diff utest : 0.11637222321064714\n",
      "----\n",
      "New Surp Human Mean :  0.7419202824306748\n",
      "New Surp LLM Mean :  0.9725264050768997\n",
      "New Surp ttest:  0.023363996186189024\n",
      "New Surp  utest : 0.009055456664712636\n",
      "----\n",
      "Dist Surp Human Mean :  0.45504828824402965\n",
      "Dist Surp LLM Mean :  0.5538503349305814\n",
      "Dist Surp ttest:  0.04689223096382202\n",
      "Dist Surp  utest : 0.031492367151090495\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [19:04<00:00,  2.31s/it]\n",
      "/home/mila/f/florian.carichon/.conda/envs/work0/lib/python3.10/site-packages/scipy/_lib/deprecation.py:234: SmallSampleWarning: After omitting NaNs, one or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Layer :  layer_1\n",
      "Newness Human Mean :  0.0009419510265975663\n",
      "Newness LLM Mean :  0.003409458833577338\n",
      "Newness ttest:  0.004242358685776614\n",
      "Newness utest : 0.013457526330308792\n",
      "----\n",
      "Uniq Human Mean :  0.5529131817855268\n",
      "Uniq LLM Mean :  0.6516804164622823\n",
      "Uniq ttest:  0.023685499535076135\n",
      "Uniq utest : nan\n",
      "----\n",
      "Diff Human Mean :  0.9334716308238363\n",
      "Diff LLM Mean :  0.9735222177944636\n",
      "Diff ttest:  nan\n",
      "Diff utest : 0.5190570648300906\n",
      "----\n",
      "New Surp Human Mean :  0.8181512920839206\n",
      "New Surp LLM Mean :  0.9870442337451819\n",
      "New Surp ttest:  0.026969456282643543\n",
      "New Surp  utest : 0.012706562418355523\n",
      "----\n",
      "Dist Surp Human Mean :  0.28304356485809407\n",
      "Dist Surp LLM Mean :  0.1953500109538496\n",
      "Dist Surp ttest:  0.13361688863817162\n",
      "Dist Surp  utest : 0.17255444477977644\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [18:52<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Layer :  layer_23\n",
      "Newness Human Mean :  0.0010643778136261897\n",
      "Newness LLM Mean :  0.0051106749991520935\n",
      "Newness ttest:  0.0014023832991076571\n",
      "Newness utest : 0.013482648477577436\n",
      "----\n",
      "Uniq Human Mean :  0.5973539371859803\n",
      "Uniq LLM Mean :  0.6275661743417502\n",
      "Uniq ttest:  0.0815850218124972\n",
      "Uniq utest : nan\n",
      "----\n",
      "Diff Human Mean :  0.9405428108473294\n",
      "Diff LLM Mean :  0.9701888545097153\n",
      "Diff ttest:  nan\n",
      "Diff utest : 0.5989107214775764\n",
      "----\n",
      "New Surp Human Mean :  0.8705937428578334\n",
      "New Surp LLM Mean :  0.9917936086407145\n",
      "New Surp ttest:  0.04248961209470889\n",
      "New Surp  utest : 0.01141051753351503\n",
      "----\n",
      "Dist Surp Human Mean :  0.2680648836070193\n",
      "Dist Surp LLM Mean :  0.3640595332303959\n",
      "Dist Surp ttest:  0.21587842659011242\n",
      "Dist Surp  utest : 0.21706480041141166\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [18:52<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Layer :  layer_-2\n",
      "Newness Human Mean :  0.0008531328599506567\n",
      "Newness LLM Mean :  0.0034962887824550853\n",
      "Newness ttest:  0.0031419984852535596\n",
      "Newness utest : 0.014831691197950773\n",
      "----\n",
      "Uniq Human Mean :  0.6679513104036536\n",
      "Uniq LLM Mean :  0.6597347410385719\n",
      "Uniq ttest:  0.29578763764284044\n",
      "Uniq utest : nan\n",
      "----\n",
      "Diff Human Mean :  0.9408154211782239\n",
      "Diff LLM Mean :  0.9683234249202478\n",
      "Diff ttest:  nan\n",
      "Diff utest : 0.6341576790455671\n",
      "----\n",
      "New Surp Human Mean :  0.8594606742419617\n",
      "New Surp LLM Mean :  0.9833404893764212\n",
      "New Surp ttest:  0.052407494327573925\n",
      "New Surp  utest : 0.02398747642585134\n",
      "----\n",
      "Dist Surp Human Mean :  0.05399981771686312\n",
      "Dist Surp LLM Mean :  0.1837505278748264\n",
      "Dist Surp ttest:  0.056718440450921136\n",
      "Dist Surp  utest : 0.09714053524378315\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [18:50<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Layer :  layer_-1\n",
      "Newness Human Mean :  0.0012704002154763233\n",
      "Newness LLM Mean :  0.0041115722203914525\n",
      "Newness ttest:  0.0032452282252366516\n",
      "Newness utest : 0.0235358032602019\n",
      "----\n",
      "Uniq Human Mean :  0.513799606057617\n",
      "Uniq LLM Mean :  0.5867567593704568\n",
      "Uniq ttest:  0.03526542755222317\n",
      "Uniq utest : nan\n",
      "----\n",
      "Diff Human Mean :  0.9257770067711099\n",
      "Diff LLM Mean :  0.9715820028953117\n",
      "Diff ttest:  nan\n",
      "Diff utest : 0.44210794583542706\n",
      "----\n",
      "New Surp Human Mean :  0.8146929084744072\n",
      "New Surp LLM Mean :  0.9723437140327339\n",
      "New Surp ttest:  0.03263823753867098\n",
      "New Surp  utest : 0.01588391947695492\n",
      "----\n",
      "Dist Surp Human Mean :  0.4170380136260847\n",
      "Dist Surp LLM Mean :  0.4770221292107046\n",
      "Dist Surp ttest:  0.25713332539093997\n",
      "Dist Surp  utest : 0.2098873418001642\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [19:05<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Layer :  layer_last\n",
      "Newness Human Mean :  0.0019601264446765083\n",
      "Newness LLM Mean :  0.005390251806595192\n",
      "Newness ttest:  0.002691304093345355\n",
      "Newness utest : 0.02429453926420589\n",
      "----\n",
      "Uniq Human Mean :  0.4121506927475228\n",
      "Uniq LLM Mean :  0.5163350151037354\n",
      "Uniq ttest:  0.03258382313621395\n",
      "Uniq utest : nan\n",
      "----\n",
      "Diff Human Mean :  0.5707101333641506\n",
      "Diff LLM Mean :  0.9341771612306022\n",
      "Diff ttest:  nan\n",
      "Diff utest : 0.10834389998628764\n",
      "----\n",
      "New Surp Human Mean :  0.6614846274550054\n",
      "New Surp LLM Mean :  0.9007158627242343\n",
      "New Surp ttest:  0.03063087751062413\n",
      "New Surp  utest : 0.03334963116693107\n",
      "----\n",
      "Dist Surp Human Mean :  0.4328478801751277\n",
      "Dist Surp LLM Mean :  0.5084743782123194\n",
      "Dist Surp ttest:  0.13126613354487277\n",
      "Dist Surp  utest : 0.08700148319302618\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For the variation countries\n",
    "for layer in layer_list:\n",
    "    dict_values = {}\n",
    "    for k in tqdm(range(len(filenames))):\n",
    "        recette = filenames[k]\n",
    "        file_path = data_path + recette\n",
    "        with open(file_path) as json_file:\n",
    "            recipe_dict = json.load(json_file)\n",
    "\n",
    "        varia_key_list = list(recipe_dict['LLM_gen']['variation_novelty'].keys())\n",
    "        varia_country_list = [key.split(\"_\")[1] for key in varia_key_list]\n",
    "\n",
    "        train_recipe_ids = list(recipe_dict['Train_Variations'].keys())\n",
    "        for i in range(len(train_recipe_ids)):\n",
    "            recipe = recipe_dict['Train_Variations'][train_recipe_ids[i]]\n",
    "            country_t = recipe['country'].lower()\n",
    "            if country_t in varia_country_list:\n",
    "                if country_t not in dict_values:\n",
    "                    dict_values[country_t] = {'LLM_new':[], 'Human_new': [], 'LLM_uniq':[], 'Human_uniq': [],'LLM_diff':[], 'Human_diff': [],'LLM_nsurp':[], 'Human_nsurp': [],'LLM_dsurp':[], 'Human_dsurp': [],}\n",
    "                dict_values[country_t]['Human_new'].append(recipe[layer]['newness_div'])\n",
    "                dict_values[country_t]['Human_uniq'].append(recipe[layer]['uniq_dist'])\n",
    "                dict_values[country_t]['Human_diff'].append(recipe[layer]['diff_local'])\n",
    "                dict_values[country_t]['Human_nsurp'].append(recipe[layer]['new_surprise'])\n",
    "                dict_values[country_t]['Human_dsurp'].append(recipe[layer]['dist_surprise'])\n",
    "                \n",
    "        valid_recipe_ids = list(recipe_dict['Valid_Variations'].keys())\n",
    "        for i in range(len(valid_recipe_ids)):\n",
    "            recipe = recipe_dict['Valid_Variations'][valid_recipe_ids[i]]\n",
    "            country_v = recipe['country'].lower()\n",
    "            if country_v in varia_country_list:\n",
    "                if country_v not in dict_values:\n",
    "                    dict_values[country_v] = {'LLM_new':[], 'Human_new': [], 'LLM_uniq':[], 'Human_uniq': [],'LLM_diff':[], 'Human_diff': [],'LLM_nsurp':[], 'Human_nsurp': [],'LLM_dsurp':[], 'Human_dsurp': [],}\n",
    "                dict_values[country_v]['Human_new'].append(recipe[layer]['newness_div'])\n",
    "                dict_values[country_v]['Human_uniq'].append(recipe[layer]['uniq_dist'])\n",
    "                dict_values[country_v]['Human_diff'].append(recipe[layer]['diff_local'])\n",
    "                dict_values[country_v]['Human_nsurp'].append(recipe[layer]['new_surprise'])\n",
    "                dict_values[country_v]['Human_dsurp'].append(recipe[layer]['dist_surprise'])\n",
    "\n",
    "        test_recipe_ids = list(recipe_dict['Test_Variations'].keys())\n",
    "        for i in range(len(test_recipe_ids)):\n",
    "            recipe = recipe_dict['Test_Variations'][test_recipe_ids[i]]\n",
    "            country_te = recipe['country'].lower()\n",
    "            if country_te in varia_country_list:\n",
    "                if country_te not in dict_values:\n",
    "                    dict_values[country_te] = {'LLM_new':[], 'Human_new': [], 'LLM_uniq':[], 'Human_uniq': [],'LLM_diff':[], 'Human_diff': [],'LLM_nsurp':[], 'Human_nsurp': [],'LLM_dsurp':[], 'Human_dsurp': [],}\n",
    "                dict_values[country_te]['Human_new'].append(recipe[layer]['newness_div'])\n",
    "                dict_values[country_te]['Human_uniq'].append(recipe[layer]['uniq_dist'])\n",
    "                dict_values[country_te]['Human_diff'].append(recipe[layer]['diff_local'])\n",
    "                dict_values[country_te]['Human_nsurp'].append(recipe[layer]['new_surprise'])\n",
    "                dict_values[country_te]['Human_dsurp'].append(recipe[layer]['dist_surprise'])\n",
    "        ## LLMs\n",
    "        for i in range(len(varia_key_list)):\n",
    "            country = varia_country_list[i]\n",
    "            if country not in dict_values:\n",
    "                dict_values[country] = {'LLM_new':[], 'Human_new': [], 'LLM_uniq':[], 'Human_uniq': [],'LLM_diff':[], 'Human_diff': [],'LLM_nsurp':[], 'Human_nsurp': [],'LLM_dsurp':[], 'Human_dsurp': [],}\n",
    "            recipe_keys = list(recipe_dict['LLM_gen']['variation_novelty'][varia_key_list[i]].keys())\n",
    "            if len(recipe_keys) > 0:\n",
    "                recipe = recipe_dict['LLM_gen']['variation_novelty'][varia_key_list[i]]\n",
    "                ### The split for the keys novel and tradi keywords should happen here\n",
    "                for key in recipe_keys:\n",
    "                    is_eng = is_english(recipe[key]['Instructions'])\n",
    "                    if is_eng:\n",
    "                        count_ = count_tokens(recipe[key]['Instructions'])\n",
    "                        if count_:\n",
    "                            dict_values[country]['LLM_new'].append(recipe[key][layer]['newness_div'])\n",
    "                            dict_values[country]['LLM_uniq'].append(recipe[key][layer]['uniq_dist'])\n",
    "                            dict_values[country]['LLM_diff'].append(recipe[key][layer]['diff_local'])\n",
    "                            dict_values[country]['LLM_nsurp'].append(recipe[key][layer]['new_surprise'])\n",
    "                            dict_values[country]['LLM_dsurp'].append(recipe[key][layer]['dist_surprise'])\n",
    "    \n",
    "    countries = list(dict_values.keys())\n",
    "    gen_mean_x0, gen_mean_y0, gen_mean_x1, gen_mean_y1, gen_mean_x2, gen_mean_y2, gen_mean_x3, gen_mean_y3, gen_mean_x4, gen_mean_y4 = [], [], [], [], [], [], [], [], [], []\n",
    "    gen_ttest_pvalue0, gen_ttest_pvalue1, gen_ttest_pvalue2, gen_ttest_pvalue3, gen_ttest_pvalue4 = [], [], [], [], []\n",
    "    gen_utest_pval0, gen_utest_pval1, gen_utest_pval2, gen_utest_pval3, gen_utest_pval4 = [], [], [], [], []\n",
    "    for country in countries:\n",
    "        x0 = np.array(dict_values[country]['Human_new'])\n",
    "        y0 = np.array(dict_values[country]['LLM_new'])\n",
    "        x1 = np.array(dict_values[country]['Human_uniq'])\n",
    "        y1 = np.array(dict_values[country]['LLM_uniq'])\n",
    "        x2 = np.array(dict_values[country]['Human_diff'])\n",
    "        y2 = np.array(dict_values[country]['LLM_diff'])\n",
    "        x3 = np.array(dict_values[country]['Human_nsurp'])\n",
    "        y3 = np.array(dict_values[country]['LLM_nsurp'])\n",
    "        x4 = np.array(dict_values[country]['Human_dsurp'])\n",
    "        y4 = np.array(dict_values[country]['LLM_dsurp'])\n",
    "\n",
    "        # Step 1: Mean values\n",
    "        gen_mean_x0.append(np.mean(x0))\n",
    "        gen_mean_y0.append(np.mean(y0))\n",
    "        gen_mean_x1.append(np.mean(x1))\n",
    "        gen_mean_y1.append(np.mean(y1))\n",
    "        gen_mean_x2.append(np.mean(x2))\n",
    "        gen_mean_y2.append(np.mean(y2))\n",
    "        gen_mean_x3.append(np.mean(x3))\n",
    "        gen_mean_y3.append(np.mean(y3))\n",
    "        gen_mean_x4.append(np.mean(x4))\n",
    "        gen_mean_y4.append(np.mean(y4))\n",
    "\n",
    "        # Step 2a: Independent t-test (Welch’s by default with equal_var=False)\n",
    "        _, p_val_t0 = stats.ttest_ind(x0, y0, equal_var=False, nan_policy=\"omit\")\n",
    "        if not np.isnan(p_val_t0):\n",
    "            gen_ttest_pvalue0.append(p_val_t0)\n",
    "        _, p_val_t1 = stats.ttest_ind(x1, y1, equal_var=False, nan_policy=\"omit\")\n",
    "        if not np.isnan(p_val_t1):\n",
    "            gen_ttest_pvalue1.append(p_val_t1)\n",
    "        _, p_val_t2 = stats.ttest_ind(2, y2, equal_var=False, nan_policy=\"omit\")\n",
    "        if not np.isnan(p_val_t2):\n",
    "            gen_ttest_pvalue2.append(p_val_t2)\n",
    "        _, p_val_t3 = stats.ttest_ind(x3, y3, equal_var=False, nan_policy=\"omit\")\n",
    "        if not np.isnan(p_val_t3):\n",
    "            gen_ttest_pvalue3.append(p_val_t3)\n",
    "        _, p_val_t4 = stats.ttest_ind(x4, y4, equal_var=False, nan_policy=\"omit\")\n",
    "        if not np.isnan(p_val_t4):\n",
    "            gen_ttest_pvalue4.append(p_val_t4)\n",
    "\n",
    "        # Step 2b: Mann–Whitney U test\n",
    "        u_stat0, p_val_u0 = stats.mannwhitneyu(x0, y0, alternative='two-sided')\n",
    "        gen_utest_pval0.append(p_val_u0)\n",
    "        u_stat1, p_val_u1 = stats.mannwhitneyu(x1, y1, alternative='two-sided')\n",
    "        gen_utest_pval1.append(p_val_u1)\n",
    "        u_stat2, p_val_u2 = stats.mannwhitneyu(x2, y2, alternative='two-sided')\n",
    "        gen_utest_pval2.append(p_val_u2)\n",
    "        u_stat3, p_val_u3 = stats.mannwhitneyu(x3, y3, alternative='two-sided')\n",
    "        gen_utest_pval3.append(p_val_u3)\n",
    "        u_stat, p_val_u4 = stats.mannwhitneyu(x4, y4, alternative='two-sided')\n",
    "        gen_utest_pval4.append(p_val_u4)\n",
    "    print('===================================')\n",
    "    print('Layer : ', layer)\n",
    "    print('Newness Human Mean : ', np.mean(gen_mean_x0))\n",
    "    print('Newness LLM Mean : ', np.mean(gen_mean_y0))\n",
    "    print('Newness ttest: ',np.mean(gen_ttest_pvalue0))\n",
    "    print('Newness utest :',np.mean(gen_utest_pval0))\n",
    "    print('----')\n",
    "    print('Uniq Human Mean : ',np.nanmean(gen_mean_x1))\n",
    "    print('Uniq LLM Mean : ',np.nanmean(gen_mean_y1))\n",
    "    print('Uniq ttest: ',np.mean(gen_ttest_pvalue1))\n",
    "    print('Uniq utest :',np.mean(gen_utest_pval1))\n",
    "    print('----')\n",
    "    print('Diff Human Mean : ',np.nanmean(gen_mean_x2))\n",
    "    print('Diff LLM Mean : ',np.nanmean(gen_mean_y2))\n",
    "    print('Diff ttest: ',np.mean(gen_ttest_pvalue2))\n",
    "    print('Diff utest :',np.mean(gen_utest_pval2))\n",
    "    print('----')\n",
    "    print('New Surp Human Mean : ',np.nanmean(gen_mean_x3))\n",
    "    print('New Surp LLM Mean : ',np.nanmean(gen_mean_y3))\n",
    "    print('New Surp ttest: ',np.mean(gen_ttest_pvalue3))\n",
    "    print('New Surp  utest :',np.mean(gen_utest_pval3))\n",
    "    print('----')\n",
    "    print('Dist Surp Human Mean : ',np.nanmean(gen_mean_x4))\n",
    "    print('Dist Surp LLM Mean : ',np.nanmean(gen_mean_y4))\n",
    "    print('Dist Surp ttest: ',np.mean(gen_ttest_pvalue4))\n",
    "    print('Dist Surp  utest :',np.mean(gen_utest_pval4))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGGREGATED VALUES AND T-TEST\n",
    "countries = list(dict_values.keys())\n",
    "gen_mean_x, gen_mean_y = [], []\n",
    "gen_ttest_pvalue = []\n",
    "gen_utest_pval = []\n",
    "for country in countries:\n",
    "    x = np.array(dict_values[country]['Human_new'])\n",
    "    y = np.array(dict_values[country]['LLM_new'])\n",
    "\n",
    "    # Step 1: Mean values\n",
    "    gen_mean_x.append(np.mean(x))\n",
    "    gen_mean_y.append(np.mean(y))\n",
    "\n",
    "    # Step 2a: Independent t-test (Welch’s by default with equal_var=False)\n",
    "    t_stat, p_val_t = stats.ttest_ind(x, y, equal_var=False, nan_policy=\"omit\")\n",
    "    if not np.isnan(p_val_t):\n",
    "        gen_ttest_pvalue.append(p_val_t)\n",
    "\n",
    "    # Step 2b: Mann–Whitney U test\n",
    "    u_stat, p_val_u = stats.mannwhitneyu(x, y, alternative='two-sided')\n",
    "    gen_utest_pval.append(p_val_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "462070dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002805420252838654\n",
      "0.0046722702810057795\n",
      "0.022089559340476506\n",
      "0.20241767304864186\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(gen_mean_x))\n",
    "print(np.mean(gen_mean_y))\n",
    "print(np.mean(gen_ttest_pvalue))\n",
    "print(np.mean(gen_utest_pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08919868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
