import json
import os
from os import walk
import spacy
nlp = spacy.load("en_core_web_sm")
import re
from tqdm import tqdm
import argparse

from LLM_Novelty_Utils import *

#############################################################################################
##############################   PARSER ARGUMENTS   #########################################
parser = argparse.ArgumentParser(description="GlobalFusion V2 Generator -- With LLMs Novelty -- Per Model Generation")


#### CHANGE AND ADAPT PATH FILE HERE
parser.add_argument('--model_gen_path', default=f'Generated_answers_qwen_2_run_0config_1/', help='Directory Name for the model to generate dataset')
parser.add_argument('--gen_path', default=f'./Gen_recipes/', help='Path to directory containing the human original recipes')
parser.add_argument('--orig_path', default=f'./GlobalFusion/', help='Path to directory containing the human original recipes')
parser.add_argument('--save_path', default=f'./GlobalFusion_withLLMs/', help='Path to directory containing the human original recipes')
parser.add_argument('--recipe_threshold', default =12, type=int, help='threshold value for filtering the train/valid/test recipes for less time in measuring novelty --- set to 100000 for basically not filtering')
args = parser.parse_args() 


#LLMs Generated recipes
gen_templates = list(set(next(walk(args.gen_path), (None,None,[]))[2]))
# Human / GlobalFusion Dataset
orig_templates = list(set(next(walk(args.orig_path), (None,None,[]))[2]))

#Checking the saving directory
save_path_model = args.save_path + args.model_gen_path
if not os.path.exists(save_path_model):
    os.makedirs(save_path_model)
else:
    ##### Preserving in gen_templates only the file that are not yet produced -- Like this you can split the generation in multiple iterations
    saved_templates = list(set(next(walk(save_path_model), (None,None,[]))[2]))
    short_list = []
    for name in gen_templates:
        match = re.match(r'^(.+?_[^_]+?_(\d+))(?=(_|\.|$))', name)
        recipe_name = match.group(1)
        short_list.append(recipe_name)
    
    filtered_short = [item for item in short_list if not any(name.startswith(item) for name in saved_templates)]
    gen_templates = [item for item in gen_templates if any(item.startswith(base) for base in filtered_short)]


################################################################################################################
##############################   GENERATING CLEAN EXPLOITABLE DATASET  #########################################
for i in tqdm(range(len(gen_templates))):    
    recipe_ = gen_templates[i]
    #Saving dictionary for the final results 
    clean_dict = {}
    clean_dict['gen_novelty'] = {}

    #Loading the dictionary with the recipe generated by the LLMs
    recipe_path = args.gen_path + recipe_
    with open(recipe_path) as json_file:
        gen_recipe_dict = json.load(json_file)
    
    #For loop in the variation countries -- extract country name from variations_country?
    country_keys = list(gen_recipe_dict.keys())
    for k, key_c in enumerate(country_keys):
        #### Setting up a limit since we don't need more than 10 -- estimationg novelty afterward for each layers is too long -- 1 hours if 35 variations
        if k <= 11:                                          ## 1 for the general one -- always comes up first and 10 variations max
            novel_keys = list(gen_recipe_dict[key_c].keys())
            for key_no in novel_keys:
                if key_no != 'country':
                    recipe = gen_recipe_dict[key_c][key_no]['answer']
                    match_ingredients, pos_ingredients = analyze_text_ingredient(recipe)
                    if match_ingredients:
                        clean_dict.setdefault(key_c, {})[key_no] = {}
                        if analyze_text_instruction(recipe, pos_ingredients):
                            results_dict_novel = split_text_sequentially(recipe)
                            clean_dict[key_c][key_no]['Title'] = clean_title(results_dict_novel['Title'])
                            split_ingr_novel = clean_ingr(results_dict_novel['Ingredients']) ###### List
                            clean_dict[key_c][key_no]['Ingredients'] = split_ingr_novel
                            clean_instructions_novel = text_cleaning(results_dict_novel['Instructions'])
                            clean_dict[key_c][key_no]['Instructions'] = clean_instructions_novel


    for key_c in list(clean_dict.keys()):
        for key_no in list(clean_dict[key_c].keys()):
            # Check if 'Instructions' is missing or empty
            if not clean_dict[key_c][key_no].get('Instructions'):
                del clean_dict[key_c][key_no]
        # Optionally remove outer dicts that became empty
        if not clean_dict[key_c]:
            del clean_dict[key_c]

    ####  LOADING THE OLD RECIPES WITH THE BASE AND HUMAN RECIPES TO ADD THE LLM Field
    short_name, orig_file = file_matching(orig_templates, recipe_)

    recipe_orig = args.orig_path + orig_file
    with open(recipe_orig) as json_file:
        orig_dict = json.load(json_file)

    ########### SAVING THE NEW FIELDS -- CLEANED AND IN PROPER FORMATIN THE OLD FILE  FOR NOVELTY ANALYSES 
    if args.gen_type == 'dist':
        orig_dict['LLM_gen'] = {}
        orig_dict['LLM_gen']['same_country_novelty'] = clean_dict['gen_novelty']
        orig_dict['LLM_gen']['cult_novelty'] = {}
        orig_dict['LLM_gen']['cult_novelty']['inglehar_welzel'] = clean_dict['inglehar_welzel']
        orig_dict['LLM_gen']['cult_novelty']['geographical'] = clean_dict['geographical']
        orig_dict['LLM_gen']['cult_novelty']['linguistic'] = clean_dict['linguistic']
        orig_dict['LLM_gen']['cult_novelty']['religious'] = clean_dict['religious']
    elif args.gen_type == 'same':
        orig_dict['LLM_gen'] = {}
        # Add country_origin only if it exists after cleanup
        if 'country_origin' in clean_dict:
            orig_dict['LLM_gen']['same_country_novelty'] = clean_dict['country_origin']
        # Always add variation_novelty (excluding unwanted keys and empty ones)
        orig_dict['LLM_gen']['variation_novelty'] = { k: v for k, v in clean_dict.items() if k not in ('country_origin', 'gen_novelty') and v}
    

    ######### Filtering the number of recipes in train/variation/test in the file to not overload the measurement process afterward
    ###Isolating countries presents in the variations to keep also the one that are pairing
    LLM_dict = orig_dict['LLM_gen']
    variations = list(orig_dict['LLM_gen']['variation_novelty'].keys())
    country_varia_list = []
    for varation in variations:
        country_varia_list.append(varation.split("_", 1)[1])
    country_varia_list.append(orig_dict['Country'].lower())   ## apending the country of origin as additional country -- otherwise it does not work...

    ###Filtering the recipes
    country_dict = {}   ### I keep only 10 total recipes per country presnt in the overall dataset --- that's it
    data_keys = ['Train_Variations', 'Valid_Variations', 'Test_Variations']
    for data_k in data_keys:
        data_recipes = orig_dict[data_k]
        data_ids = list(data_recipes.keys())
        for id_k in data_ids:
            country = data_recipes[id_k]['country'].lower()
            if country not in country_varia_list:
                del orig_dict[data_k][id_k]
            else:
                if country not in country_dict.keys():
                    country_dict[country] = 1
                else:
                    if country_dict[country] <= args.recipe_threshold:
                        country_dict[country] += 1
                    else:
                        del orig_dict[data_k][id_k]
    


    ## Defining saving path
    save_name = save_path_model +  f"{short_name}_full.json"
    with open(save_name, "w", encoding="utf-8") as f:
        json.dump(orig_dict, f, indent=4, ensure_ascii=False)